{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the patsy library\n",
    "Developing interaction factors is a pretty common feature of many data science platforms and this can of course be done with Python as well.  The issue is that this requires a lot of `for` loops or recursive functions to develop all the interactions with the various variables. \n",
    "\n",
    "(from the [patsy documentation](https://patsy.readthedocs.io/en/latest/overview.html))\n",
    ">`patsy` is a Python package for describing statistical models (especially linear models, or models that have a linear component) and building design matrices. It is closely inspired by and compatible with the formula mini-language used in R and S.\n",
    ">\n",
    ">For instance, if we have some variable y, and we want to regress it against some other variables x, a, b, and the interaction of a and b, then we simply write:\n",
    ">\n",
    ">```patsy.dmatrices(\"y ~ x + a + b + a:b\", data)```\n",
    ">\n",
    "> and Patsy takes care of building appropriate matrices. Furthermore, it:\n",
    "> \n",
    "> * Allows data transformations to be specified using arbitrary Python code: instead of `x`, we could have written `log(x)`, `(x > 0)`, or even `log(x) if x > 1e-5 else log(1e-5)`,\n",
    "> * Provides a range of convenient options for coding categorical variables, including automatic detection and removal > of redundancies,\n",
    "> * Knows how to apply ‘the same’ transformation used on original data to new data, even for tricky transformations like > centering or standardization (critical if you want to use your model to make predictions),\n",
    "> * Has an incremental mode to handle data sets which are too large to fit into memory at one time,\n",
    "> * Provides a language for symbolic, human-readable specification of linear constraint matrices,\n",
    "> * Has a thorough test suite (>97% statement coverage) and solid underlying theory, allowing it to correctly handle > corner cases that even R gets wrong, and\n",
    "> * Features a simple API for integration into statistical packages.\n",
    "> \n",
    "> What Patsy won’t do is, well, statistics — it just lets you describe models in general terms. It doesn’t know or \n",
    "> care whether you ultimately want to do linear regression, time-series analysis, or fit a forest of decision trees,\n",
    ">  and it certainly won’t do any of those things for you — it just gives a high-level language for describing which \n",
    "> factors you want your underlying model to take into account. It’s not suitable for implementing arbitrary non-linear \n",
    "> models from scratch; for that, you’ll be better off with something like Theano, SymPy, or just plain Python. \n",
    "> But if you’re using a statistical package that requires you to provide a raw model matrix, then you can use Patsy to painlessly construct that model matrix; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from patsy.highlevel import dmatrices\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data import load_excel, TRUE_VALUES, FALSE_VALUES\n",
    "from src.metric import confusion_matrix,classificationSummary\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pd.set_option('precision',4)\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean-up the heart disease dataset\n",
    "heart_df = load_excel('HeartDisease_Cleveland',\n",
    "                    dtype={'FBS': bool, 'EXANG': bool}, true_values=TRUE_VALUES,\n",
    "                    false_values=FALSE_VALUES, na_values=['?'])\n",
    "heart_df.dropna(inplace=True)\n",
    "heart_df['DIAG'] = (heart_df.NUM > 0)\n",
    "heart_df.drop(columns=['NUM'], inplace=True)\n",
    "# Set the MAX_NEIGHBORS\n",
    "MAX_NEIGHBORS = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_shape:(297, 19), y_shape:(297, 1)\n"
     ]
    }
   ],
   "source": [
    "# This function will build the matrices (that is the predictors with interactions)\n",
    "def setup_matricies(df):\n",
    "\n",
    "    # We'll look for DIAG as the target and \n",
    "    # Define SEX, CP, FBS, RESTECG, EXANG, SLOPE and THAL as categorical variables\n",
    "    # The other predictors are already seen as numeric\n",
    "    y, X = dmatrices('DIAG ~ AGE + C(SEX) + C(CP) + C(FBS) + TRESTBPS + CHOL +'\n",
    "                         'C(RESTECG) + THALACH + C(EXANG) + OLDPEAK + C(SLOPE) +'\n",
    "                         'CA + C(THAL) - 1', df, return_type='dataframe')\n",
    "\n",
    "    # The y (target) matrix uses dummy encoding for both True and False values, \n",
    "    #  since we only need to know if TRUE or not we can drop it\n",
    "    y.drop(columns=['DIAG[False]'],inplace=True)\n",
    "    y.rename(columns={'DIAG[True':'DIAG'}, inplace=True)\n",
    "\n",
    "    # For convenience we'll rename all the other columns so that we can read them\n",
    "    X.rename(columns={'C(SEX)[T.1]':'Male','C(CP)[T.2]':'CP_Atypical',\n",
    "                      'C(CP)[T.3]':'CP_NonAngina', 'C(CP)[T.4]':'CP_Asymptomatic',\n",
    "                      'C(FBS)[T.True]':'FBS_True', 'C(RESTECG)[T.1]':'RESTECG_1',\n",
    "                      'C(RESTECG)[T.2]':'RESTECG_2', 'C(EXANG)[T.1]':'EXANG_True',\n",
    "                      'C(SLOPE)[T.2]':'SLOPE_Flat', 'C(SLOPE)[T.3]':'SLOPE_Down',\n",
    "                      'C(THAL)[T.6.0]':'THAL_FIXED', 'C(THAL)[T.7.0]':'THAL_REV'},\n",
    "            inplace=True)\n",
    "\n",
    "    print(f'X_shape:{X.shape}, y_shape:{y.shape}')\n",
    "    y = np.ravel(y)\n",
    "    return X, y\n",
    "\n",
    "# It's convenient to have the preceding in a function so that we can call it again later if needed\n",
    "X, y = setup_matricies(heart_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by setting the random_state variable, we can ensure that each time we run the method \n",
    "# we get the same splits\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best for K (simple) = 7 Score: 0.663866\n"
     ]
    }
   ],
   "source": [
    "# Now let's see if we can find a good model just by varying the values of K\n",
    "best_score = 0\n",
    "best_k = 0\n",
    "clf = None\n",
    "for K in range(1, MAX_NEIGHBORS, 2):\n",
    "    # We are only going to change the number of neighbors on each pass to see if it helps\n",
    "    clf = KNeighborsClassifier(n_neighbors=K, weights='uniform', algorithm='auto', n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    score = accuracy_score(y_valid, y_pred)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = K\n",
    "        best_y_pred = y_pred\n",
    "print(f'Best for K (simple) = {best_k} Score: {best_score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "Another technique that helps us to ensure that we are not overfitting is to take several passes at the data and leave out some percentage.  For instance, if we have 5 passes, we split the data into 5 groups.  In pass one, we use groups 1-4 to train the model and group 5 to validate.  Then we train on sets 1-3 and 5 and validate against 4.  Each time leaving 1/5 of the data to validate our model.  When we are done, we take the average of each of the splits and hopefully have a better model.\n",
    "\n",
    "We can cross-validate in a number of ways:\n",
    "* Leave one out cross validation\n",
    "* k-fold cross validation\n",
    "* Stratified k-fold cross validation\n",
    "* Time Series cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_c_val = 0\n",
    "best_k = 0\n",
    "best_model = None\n",
    "for n in range(1, MAX_NEIGHBORS, 2):\n",
    "    clf = KNeighborsClassifier(n_neighbors=n)\n",
    "    c_val = cross_val_score(clf, X, y, cv=5, scoring='accuracy').mean()\n",
    "    if c_val > best_c_val:\n",
    "        best_c_val = c_val\n",
    "        best_k = n\n",
    "        best_model = clf\n",
    "print(f'Best for K (cross_val) = {best_k} Score: {best_c_val:.6f}')\n",
    "return best_model"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
