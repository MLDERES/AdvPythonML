{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn import datasets\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.metric import classificationSummary, confusion_matrix\n",
    "\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "One of the most straightforward machine learning techniques is classification.  That is determining which category a to which particular observation belongs.  For example: \n",
    "* an individual passenger on the titanic was likely to survive,\n",
    "* which income bracket an individual is likely to be a part of \n",
    "* will a customer will qualify for a loan.\n",
    "\n",
    "All of these are classification questions.  Given a set of independent variables (e.g. factors) what is the likelihood that the dependent variable meets that category. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Predictive Performance\n",
    "Before we get too far into running algorithms we need a method to determine how good our model is.  There are a number of different metrics we might use.  But this depends on whether we are predicting/estimating a continuous target or a categorical target.  Some of the common methods are included here.\n",
    "\n",
    "Continuous Target\n",
    "* MAE - Mean absolute error\n",
    "* Mean Error - \n",
    "* MPE - Mean percentage error\n",
    "* MAPE - Mean absolute percentage error\n",
    "* RMSE - Root mean square error\n",
    "\n",
    "We will look at the continuous methods in depth in the [Estimation Notebook](04-EstimationPrediction.ipynb).\n",
    "\n",
    "Categorical Target\n",
    "- Misclassification Rate\n",
    "- Recall (aka True Positive Rate, hit rate, sensitivity)\n",
    "- Precision\n",
    "\n",
    "We'll focus here on _accuracy_ \n",
    "\n",
    "$$accuracy = \\frac{NumberOfCorrectPredictions}{AllPredictions}$$\n",
    "and _precision_\n",
    "$$precision = \\frac{TruePositives}{TruePositives+FalsePositives}$$\n",
    "\n",
    "### Naive Performance\n",
    "The simplest way to determine if a model is valuable or not is to compare to the \"naive rule\".  In essense, if we predict the most common outcome every time what is the misclassification rate.  If our model is no better than this, then our model isn't very helpful.\n",
    "\n",
    "For instance, let's say that in a list of 10 loan applicants that 8 of the applicants will be approved for a loan and that 2 won't be approved.  If we were to blindly assume that all 10 applicants would be approved for a loan - then we would be wrong 20% of the time (2/10).  Therefore, if we can't develop a model which is at least 80% accurate then we don't have a very good model.\n",
    "\n",
    "### Cut-off values for classification\n",
    "We also need to consider that in nearly all of our classification models, what we are given is the _probability_ or likelihood that the predicted target is in the given class, therefore the value we get from our classification algorithms is on a scale from 0 to 1.  Most often, the default is to say that a target belongs to a class if the probability of being in the class is > 0.5.  But it can be useful to move this cut-off.  For instance, if the cost of predicting a particular class is unusually high, we may want to raise the probability to 0.6 or greater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Logistic Regression\n",
    "Logistic regression, despite its name, is a linear model for classification rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a [logistic function](https://en.wikipedia.org/wiki/Logistic_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing a few key functions\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "delays_df = pd.read_csv(DATA_DIR/'FlightDelays.csv')\n",
    "delays_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to categorical\n",
    "delays_df.DAY_WEEK = delays_df.DAY_WEEK.astype('category')\n",
    "\n",
    "# create hourly bins departure time \n",
    "delays_df.CRS_DEP_TIME = [round(t / 100) for t in delays_df.CRS_DEP_TIME]\n",
    "delays_df.CRS_DEP_TIME = delays_df.CRS_DEP_TIME.astype('category')\n",
    "\n",
    "predictors = ['DAY_WEEK', 'CRS_DEP_TIME', 'ORIGIN', 'DEST', 'CARRIER']\n",
    "outcome = 'Flight Status'\n",
    "\n",
    "X = pd.get_dummies(delays_df[predictors])\n",
    "y = (delays_df[outcome] == 'delayed').astype(int)\n",
    "classes = ['ontime', 'delayed']\n",
    "\n",
    "# split into training and validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=1)\n",
    "\n",
    "# run naive Bayes\n",
    "delays_nb = MultinomialNB(alpha=0.01)\n",
    "delays_nb.fit(X_train, y_train)\n",
    "\n",
    "# predict probabilities\n",
    "predProb_train = delays_nb.predict_proba(X_train)\n",
    "predProb_valid = delays_nb.predict_proba(X_valid)\n",
    "\n",
    "# predict class membership\n",
    "y_valid_pred = delays_nb.predict(X_valid)\n",
    "y_train_pred = delays_nb.predict(X_train)\n",
    "\n",
    "classificationSummary(y_train,y_train_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
